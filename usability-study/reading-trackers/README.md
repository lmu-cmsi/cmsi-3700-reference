[![Review Assignment Due Date](https://classroom.github.com/assets/deadline-readme-button-22041afd0340ce965d47ae6ef1cefeee28c7c493a6346c4f15d667ab976d596c.svg)](https://classroom.github.com/a/tUIv-eYI)
**CMSI 3700** Interaction Design, Fall 2024

# Usability Study

This assignment aims to give you some firsthand experience with collecting usability measurements and assessing how well a device or system complies with its associated guidelines document. 

## Background Reading
Refer to our previous lectures for information on design thinking, principles, and further resources.
- [Design Thinking Part I](https://avartanii.notion.site/3-Design-Thinking-Part-I-0c4ccb1c424f42f791d889fc36d1e692)
- [Design Thinking Part II](https://avartanii.notion.site/4-Design-Thinking-Part-II-f2b75b88a7d2481bbc652fed6357b87a)
- [Design Thinking Part III](https://avartanii.notion.site/5-Design-Thinking-Part-III-62593eab354e4e3b854404c198ec7a6d)
- [Principles of Design Part I](https://avartanii.notion.site/6-Principles-of-Design-Part-I-04a335da57f541d98c82253716c7c829)
- [Principles of Design Part II](https://avartanii.notion.site/7-Principles-of-Design-Part-II-e1277495e4714005aab52bac55a8105e)
- [Human Abilities Part I](https://avartanii.notion.site/8-Human-Abilities-Part-I-92af177c3a17418e98e627c2c0dbb78c)
- [Human Abilities Part II](https://avartanii.notion.site/9-Human-Abilities-Part-II-92b1bc37579e44ac90b83f7bbec48bd8)
- [Usability Evaluation Part I](https://avartanii.notion.site/10-Usability-Evaluation-Part-I-b19e5a762b15450abef88a4f9f014577)
- [Usability Evaluation Part II](https://avartanii.notion.site/11-Usability-Evaluation-Part-II-2d65f637614f4941ba958f80ca749d1c)

## For Submission

Perform a small-scale usability study comparing two systems of similar _utility_ but with differing user interfaces (_usability_). Document your study, its results, and your analysis in a report within this assignment’s repository. Use [Markdown (`.md`) format](https://guides.github.com/features/mastering-markdown/) for the report. A report template is included in the repository so that you have a starting point for the document.

### Determine Systems and Organize Groups
_Due Friday Oct. 11, 2024 @ 11:59pm (5 points)_  
We will first organize ourselves into teams of up to four (4) students, then designate a type of system or application with at least two functionally comparable products to each team.

We are using an Excel spreadsheet to get organized. Specify your group members, and the systems/applications you will be evaluating. We are targeting groups of four (4) or less; students may belong to only one group. Here is the spreadsheet link:

[Usability Study Groups](https://lmu0-my.sharepoint.com/:x:/g/personal/allen_vartanian_lmu_edu/EVxOHKfjX2VAh4olQ91Ugz0BNwo_zJBTivFWAYWAv4Ia2w?e=OpODES)

It is private, shared only with you through your student email.

To help me setup the groups in GitHub Classroom, please also include the emails associated with your GitHub accounts in the spreadsheet.

### Select Three Tasks
_Due Friday Oct. 18, 2024 @ 11:59pm (15 points)_  
Select three (3) concrete tasks that for your test subjects to perform (e.g., “place a call to (424) 555-1978”). These form the basis of your measurement activities.

_**Document these tasks in an initial draft of your report and acquire my explicit approval for them.**_ The approval will be recorded as a commit comment on your repository.

### Usability Metrics
Record usability metrics for your assigned system for an appropriately chosen cohort of users. You may take measurements from as many people as you like, classmates or otherwise. Due to our limited time and resources, we don’t require a particular number of users, but aim for at least eight (8)—that seems to be a fairly reachable number given that there are up to four members in the team.

Choose three (3) metrics from this subset—yes, that essentially means eliminating _one_ metric that you don’t think will work well for your group or your chosen system:

- _Learnability_—Remember that this only applies to subjects who are not familiar with the particular system they are about to use (interface knowledge), but understand the tasks in your list (domain knowledge). This metric is the _time_ to accomplish tasks _without prior training_.

- _Efficiency_—For this metric, use subjects who _are_ familiar with the system they are about to use (the more proficient, the better). If such subjects are hard to find, you can opt to give them some training and/or practice time in order to gain some level of expertise.

- _Errors_—Remember that in IxD, an error is not a bug, exception, nor crash, but an incident where the user does something whose result is not what he or she expected.

- _Satisfaction_—We will stay very simple here. Just ask your subjects to rate, on a scale of 1 to 10, how much they enjoyed performing each task on their respective devices or systems.

**Quick Self-Check:** What metric is not included in this list and what might be a reason for its omission?

In your submission, report the results of your studies and make a judgment call on which device or system you feel performed best. State and explain the priorities you gave to each metric.

### Heuristic Evaluation
_Due Friday Nov. 1, 2024 @ 11:59pm (70 points)_  
Explore _why_ you think your assigned systems performed the way they did. Base your discussion on one or more of the following:

- Mental model of the system from its developers’ and users’ perspectives.

- Guidelines documents that correspond or apply to your assigned devices or systems (i.e., how well [or badly] your assigned device or system complies with those guidelines).

- A well-chosen subset of interaction design principles or theories.

- The effectiveness or appropriateness of the predominant interaction style(s) used by the systems.

### Statement of Work
_Due Friday Nov. 1, 2024 @ 11:59pm (10 points)_  
End your report with a brief summary of what each group member did for the overall study and final report. It goes without saying that you should distribute the work evenly. Allocate a clear, equitable proportion of the overall study and final report to each group member. If you didn’t know it yet, you’re being told now: `git` can measure and track everyone’s contribution by commits and lines of text. Group members that contribute inequitably to the overall work will detract from the final score.

## General Tips
Be as concrete and grounded as possible. For example, you can provide screenshots from the actual system to illustrate your points. Refer to specific guideline statements or principles. Connect statements about mental models to specific artifacts of the system image (screenshots again). Et cetera.

And of course, write clearly, with the appropriate style and voice. Proofread a lot—your score will reflect both what you say and how effectively you say it.

## Specific Point Allocations

| Category | Points |
| ---- | ---- |
| Groups | 5 |
| Tasks | 15|
| Heuristic evaluation<br> - Content<br> - Writing quality | 70  <br> - 55<br> - 15 |
| Statement of work | 10 |
| **Total** | **100** |

Writing assignments are scored under the overall categories of _Content_ and _Writing_. _Content_ pertains to the outcomes listed in the syllabus for writing assignments: you want to demonstrate outcomes _1a_, _1b_, _2a_, and _2b_, up to the concepts that have been covered in class to this point. _Writing_ pertains to how well these ideas are expressed, and how cleanly. There is also a certain maturity and professionalism of voice that makes your points more compelling and authoritative. Your voice in this project should be much more formal than what is expected of your weekly design observations and critiques.
